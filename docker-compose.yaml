
services:
  # Service for the Ollama LLM server
  ollama:
    build:
      context: .
      dockerfile: ./ollama/Dockerfile                
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama                    
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=mistral


  minivault:
    build:  
      context: .
      dockerfile: ./app/Dockerfile
    restart: unless-stopped 
    ports:
      - "8000:8000" 
    volumes:
      - ./logs:/app/logs 
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=mistral 
    depends_on:
      - ollama 


volumes:
  ollama_data: 